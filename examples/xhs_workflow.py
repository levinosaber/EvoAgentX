import os
import asyncio
import textwrap
import datetime
from dotenv import load_dotenv
from evoagentx.models.openai_model import OpenAILLM
from evoagentx.models.model_configs import LLMConfig
from evoagentx.workflow import WorkFlow, WorkFlowGraph
from evoagentx.workflow.workflow_graph import WorkFlowNode, WorkFlowEdge
from evoagentx.agents import CustomizeAgent, AgentManager, ActionAgent
from evoagentx.prompts import StringTemplate, ChatTemplate
from evoagentx.models import OpenAILLMConfig
from evoagentx.tools.search_ddgs import DDGSSearchToolkit
from evoagentx.tools.image_analysis import ImageAnalysisTool
from evoagentx.tools.flux_image_generation import FluxImageGenerationTool

load_dotenv()

# éœ€è¦çš„APIå¯†é’¥
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
BFL_API_KEY = os.getenv("BFL_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENAI_ORGANIZATION_ID = os.getenv("OPENAI_ORGANIZATION_ID")

# é…ç½®LLM
llm_config = OpenAILLMConfig(
    model="gpt-4.1", 
    openai_key=OPENAI_API_KEY, 
    stream=True, 
    output_response=True
)

def create_prompt_analysis_agent():
    """
    Agent 0: Prompt Analysis Agent
    è´Ÿè´£è§£æç”¨æˆ·è¾“å…¥çš„promptï¼Œç¡®å®šç ”ç©¶ä¸»é¢˜ã€æ˜¯å¦éœ€è¦å›¾ç‰‡ç”Ÿæˆä»¥åŠå›¾ç‰‡æ•°é‡å’Œå†…å®¹
    """
    
    prompt_analysis_agent = CustomizeAgent(
        name="PromptAnalysisAgent",
        description="åˆ†æç”¨æˆ·æç¤ºè¯ï¼Œç¡®å®šç ”ç©¶ä¸»é¢˜å’Œå›¾ç‰‡ç”Ÿæˆéœ€æ±‚",
        prompt_template=ChatTemplate(
            instruction="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æç¤ºè¯åˆ†æä¸“å®¶ï¼Œè´Ÿè´£å°†ç”¨æˆ·è¯·æ±‚åˆ†è§£ä¸ºç»“æ„åŒ–çš„ç ”ç©¶å’Œå†…å®¹ç”Ÿæˆä»»åŠ¡ã€‚ä½ å¿…é¡»è¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„JSONæ ¼å¼å­—ç¬¦ä¸²ï¼ŒåŒ…å«å›¾ç‰‡æè¿°å’Œå›¾ç‰‡ä¸Šçš„æ–‡å­—å†…å®¹ã€‚è¯·ç”¨ä¸­æ–‡å›å¤æ‰€æœ‰å†…å®¹ã€‚",
            context="ä½ åˆ†æç”¨æˆ·æç¤ºè¯æ¥ç¡®å®šï¼š1) ä¸»è¦ç ”ç©¶ä¸»é¢˜ï¼Œ2) éœ€è¦ç”Ÿæˆå¤šå°‘å¼ å›¾ç‰‡ï¼Œ3) æ¯å¼ å›¾ç‰‡åº”è¯¥åŒ…å«ä»€ä¹ˆå†…å®¹ï¼Œ4) æ¯å¼ å›¾ç‰‡ä¸Šéœ€è¦æ·»åŠ ä»€ä¹ˆæ–‡å­—å†…å®¹ã€‚ä½ è¿˜åº”è¯¥ä¿ç•™åŸå§‹ç”¨æˆ·æç¤ºè¯ä¾›åç»­å¤„ç†ä½¿ç”¨ã€‚",
            constraints=[
                "å¿…é¡»è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼å­—ç¬¦ä¸²",
                "JSONç»“æ„å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šæ ¼å¼ï¼š{\"image_1\": {\"description\": \"å›¾ç‰‡æè¿°\", \"add_on\": \"å›¾ç‰‡ä¸Šçš„æ–‡å­—å†…å®¹\"}, \"image_2\": {...}}",
                "image_descriptions_jsonå­—æ®µå¿…é¡»æ˜¯ä¸€ä¸ªå¯ä»¥ç›´æ¥è¢«json.loads()è§£æçš„å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯dictå¯¹è±¡",
                "è¯·ä¸¥æ ¼æŒ‰ç…§json.dumpsçš„æ ¼å¼è¾“å‡ºimage_descriptions_jsonï¼šæœ€å¤–å±‚å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å·åŒ…è£¹ï¼Œå†…éƒ¨keyå’Œvalueä¹Ÿéƒ½ç”¨è‹±æ–‡åŒå¼•å·",
                "ä¸è¦è¾“å‡ºPythoné£æ ¼çš„å­—å…¸å¯¹è±¡æˆ–æ•°ç»„ï¼Œä¸è¦çœç•¥å¼•å·",
                "å›¾ç‰‡ç¼–å·ä»image_1å¼€å§‹é€’å¢",
                "descriptionå­—æ®µåŒ…å«å›¾ç‰‡çš„ä¸»è¦å†…å®¹æè¿°ï¼ˆåœºæ™¯ã€é£æ ¼ã€è‰²è°ƒã€æ„å›¾ç­‰ï¼‰",
                "add_onå­—æ®µä¸“é—¨åŒ…å«éœ€è¦åœ¨å›¾ç‰‡ä¸Šæ·»åŠ çš„æ–‡å­—å†…å®¹ï¼ˆæ ‡é¢˜ã€æ ‡è¯­ã€å…³é”®è¯ç­‰ï¼‰ï¼Œå¿…é¡»è¯¦ç»†è¯´æ˜ï¼š1) æ–‡å­—å†…å®¹ï¼Œ2) å­—ä½“é¢œè‰²ï¼Œ3) å­—ä½“ç²—ç»†ï¼Œ4) å¯¹é½æ–¹å¼ï¼Œ5) å­—ä½“å¤§å°ï¼ˆå å›¾ç‰‡é«˜åº¦çš„ç™¾åˆ†æ¯”ï¼‰ï¼Œ6) æ–‡å­—ä½ç½®ï¼ˆé¡¶éƒ¨ã€ä¸­å¤®ã€åº•éƒ¨ã€å·¦ä¸Šè§’ã€å³ä¸‹è§’ç­‰ï¼‰",
                "å§‹ç»ˆæä¾›æ¸…æ™°çš„ç ”ç©¶ä¸»é¢˜æ€»ç»“",
                "æ‰€æœ‰è¾“å‡ºå†…å®¹å¿…é¡»ä½¿ç”¨ä¸­æ–‡",
                "JSONå­—ç¬¦ä¸²å¿…é¡»æ˜¯æœ‰æ•ˆçš„ï¼Œå¯ä»¥è¢«json.loads()è§£æ",
                "ä¸è¦ä½¿ç”¨markdownä»£ç å—æ ¼å¼ï¼Œç›´æ¥è¿”å›çº¯JSONå­—ç¬¦ä¸²",
                "ä¸è¦æ·»åŠ ```jsonæˆ–```æ ‡è®°"
            ],
            demonstrations=[
                {
                    "input": "ä¸ºå°çº¢ä¹¦åˆ›å»ºä¸€ä¸ªå…³äºAIç”Ÿäº§åŠ›å·¥å…·çš„ä¸“ä¸šå¸–å­ï¼ŒåŒ…å«2å¼ å›¾ç‰‡",
                    "output": {
                        "research_topic": "AIç”Ÿäº§åŠ›å·¥å…·åœ¨å°çº¢ä¹¦å¹³å°çš„ä¸“ä¸šå†…å®¹åˆ›ä½œ",
                        "image_descriptions_json": "{\"image_1\": {\"description\": \"ä¸“ä¸šäººå£«åœ¨ç°ä»£åŒ–åŠå…¬ç¯å¢ƒä¸­ä½¿ç”¨AIå·¥å…·çš„åœºæ™¯ï¼ŒåŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹å’Œæ•°æ®åˆ†æç•Œé¢ï¼Œä¸“ä¸šå•†åŠ¡é£æ ¼ï¼Œæ˜äº®è‰²è°ƒï¼Œä¿¯è§†è§’åº¦æ„å›¾ï¼Œä½“ç°ç§‘æŠ€æ„Ÿå’Œæ•ˆç‡\", \"add_on\": \"åœ¨å›¾ç‰‡ä¸Šæ–¹æ·»åŠ å¤§æ ‡é¢˜ï¼š'AIç”Ÿäº§åŠ›é©å‘½'ï¼Œä½¿ç”¨ç™½è‰²å­—ä½“ï¼Œç²—ä½“ï¼Œå±…ä¸­æ˜¾ç¤ºï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„15%ã€‚åœ¨å›¾ç‰‡ä¸‹æ–¹æ·»åŠ å‰¯æ ‡é¢˜ï¼š'æå‡å·¥ä½œæ•ˆç‡ | æ™ºèƒ½åŠå…¬æ–°æ—¶ä»£'ï¼Œä½¿ç”¨æµ…è“è‰²å­—ä½“ï¼Œä¸­ç­‰ç²—ç»†ï¼Œå±…ä¸­æ˜¾ç¤ºï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„8%ã€‚åœ¨å›¾ç‰‡å³ä¸‹è§’æ·»åŠ æ ‡ç­¾ï¼š'#AIå·¥å…· #æ•ˆç‡æå‡'ï¼Œä½¿ç”¨ç™½è‰²å­—ä½“ï¼Œç»†ä½“ï¼Œå³å¯¹é½ï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„5%\"}, \"image_2\": {\"description\": \"ä¸€ä¸ªç°ä»£åŒ–çš„åŠå…¬æ¡Œï¼Œä¸Šé¢æœ‰ç¬”è®°æœ¬ç”µè„‘ã€å’–å•¡æ¯ã€ç»¿æ¤å’Œæ•´æ´çš„æ–‡ä»¶ï¼Œä½“ç°é«˜æ•ˆåŠå…¬ç¯å¢ƒï¼Œè‡ªç„¶å…‰çº¿ï¼Œæ¸©æš–è‰²è°ƒï¼Œ45åº¦è§’åº¦æ„å›¾ï¼Œçªå‡ºå·¥ä½œæ°›å›´\", \"add_on\": \"\"}}",
                        "user_prompt": "ä¸ºå°çº¢ä¹¦åˆ›å»ºä¸€ä¸ªå…³äºAIç”Ÿäº§åŠ›å·¥å…·çš„ä¸“ä¸šå¸–å­ï¼ŒåŒ…å«2å¼ å›¾ç‰‡"
                    }
                },
                {
                    "input": "ä¸ºå°çº¢ä¹¦åˆ›å»ºä¸€ä¸ªå…³äºå¥åº·æ™¨é—´ä¹ æƒ¯çš„è½»æ¾å¸–å­ï¼ŒåŒ…å«1å¼ å›¾ç‰‡",
                    "output": {
                        "research_topic": "å¥åº·æ™¨é—´ä¹ æƒ¯åœ¨å°çº¢ä¹¦å¹³å°çš„ç”Ÿæ´»æ–¹å¼å†…å®¹åˆ›ä½œ",
                        "image_descriptions_json": "{\"image_1\": {\"description\": \"å®é™çš„æ™¨é—´åœºæ™¯ï¼ŒåŒ…å«è¥å…»ä¸°å¯Œçš„æ—©é¤ï¼ˆå¦‚ç‡•éº¦ç²¥ã€æ–°é²œæ°´æœã€åšæœï¼‰ã€å†¥æƒ³è§’è½ï¼ˆç‘œä¼½å«ã€é¦™è–°èœ¡çƒ›ã€ç»¿æ¤ï¼‰ã€ä»¥åŠå¥åº·æ´»åŠ¨ï¼ˆå¦‚æ™¨è·‘è£…å¤‡ã€æ°´æ¯ã€ç»´ç”Ÿç´ ï¼‰ï¼Œæ¸©æš–è‡ªç„¶é£æ ¼ï¼ŒæŸ”å’Œè‰²è°ƒï¼Œ45åº¦è§’åº¦æ„å›¾ï¼Œä½“ç°ç”Ÿæ´»å“è´¨å’Œå¥åº·ç†å¿µ\", \"add_on\": \"åœ¨å›¾ç‰‡é¡¶éƒ¨æ·»åŠ ä¸»æ ‡é¢˜ï¼š'å¥åº·æ™¨é—´ä¹ æƒ¯'ï¼Œä½¿ç”¨æ·±ç»¿è‰²å­—ä½“ï¼Œç²—ä½“ï¼Œå±…ä¸­æ˜¾ç¤ºï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„12%ã€‚åœ¨å›¾ç‰‡ä¸­å¤®åä¸Šæ·»åŠ å‰¯æ ‡é¢˜ï¼š'ç¾å¥½ç”Ÿæ´»ä»æ—©æ™¨å¼€å§‹'ï¼Œä½¿ç”¨æµ…æ£•è‰²å­—ä½“ï¼Œä¸­ç­‰ç²—ç»†ï¼Œå±…ä¸­æ˜¾ç¤ºï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„8%ã€‚åœ¨å›¾ç‰‡å³ä¸‹è§’æ·»åŠ æ ‡ç­¾ï¼š'#å¥åº·ç”Ÿæ´» #æ™¨é—´routine'ï¼Œä½¿ç”¨ç™½è‰²å­—ä½“ï¼Œç»†ä½“ï¼Œå³å¯¹é½ï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„5%ã€‚åœ¨å›¾ç‰‡å·¦ä¸‹è§’æ·»åŠ å°æç¤ºï¼š'æ¯æ—¥åšæŒï¼Œæ”¹å˜ç”Ÿæ´»'ï¼Œä½¿ç”¨æµ…ç°è‰²å­—ä½“ï¼Œç»†ä½“ï¼Œå·¦å¯¹é½ï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„4%\"}}",
                        "user_prompt": "ä¸ºå°çº¢ä¹¦åˆ›å»ºä¸€ä¸ªå…³äºå¥åº·æ™¨é—´ä¹ æƒ¯çš„è½»æ¾å¸–å­ï¼ŒåŒ…å«1å¼ å›¾ç‰‡"
                    }
                },
                {
                    "input": "ä¸ºå°çº¢ä¹¦åˆ›å»ºä¸€ä¸ªå…³äºè¿œç¨‹å·¥ä½œæŠ€å·§çš„å¹½é»˜å¸–å­ï¼ŒåŒ…å«3å¼ å›¾ç‰‡",
                    "output": {
                        "research_topic": "è¿œç¨‹å·¥ä½œæŠ€å·§åœ¨å°çº¢ä¹¦å¹³å°çš„èŒåœºå†…å®¹åˆ›ä½œ",
                        "image_descriptions_json": "{\"image_1\": {\"description\": \"å® ç‰©æ‰“æ–­è§†é¢‘ä¼šè®®çš„æç¬‘åœºæ™¯ï¼ŒåŒ…æ‹¬çŒ«å’ªååœ¨é”®ç›˜ä¸Šã€ç‹—ç‹—å¥½å¥‡åœ°çœ‹ç€å±å¹•ã€ä»¥åŠä¸»äººæ— å¥ˆçš„è¡¨æƒ…ï¼Œè½»æ¾å¹½é»˜é£æ ¼ï¼Œæ¸©æš–è‰²è°ƒï¼Œå¹³è§†è§’åº¦æ„å›¾ï¼Œä½“ç°å±…å®¶åŠå…¬çš„çœŸå®åœºæ™¯\", \"add_on\": \"åœ¨å›¾ç‰‡é¡¶éƒ¨æ·»åŠ æ ‡é¢˜ï¼š'è¿œç¨‹å·¥ä½œçš„çœŸå®å†™ç…§'ï¼Œä½¿ç”¨æ©™è‰²å­—ä½“ï¼Œç²—ä½“ï¼Œå±…ä¸­æ˜¾ç¤ºï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„10%ã€‚åœ¨å›¾ç‰‡ä¸­å¤®æ·»åŠ å¹½é»˜æ–‡å­—ï¼š'å® ç‰©ä¹Ÿæ˜¯åŒäº‹'ï¼Œä½¿ç”¨æ·±è“è‰²å­—ä½“ï¼Œä¸­ç­‰ç²—ç»†ï¼Œå±…ä¸­æ˜¾ç¤ºï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„8%ã€‚åœ¨å›¾ç‰‡å³ä¸‹è§’æ·»åŠ æ ‡ç­¾ï¼š'#è¿œç¨‹å·¥ä½œ #å±…å®¶åŠå…¬'ï¼Œä½¿ç”¨ç™½è‰²å­—ä½“ï¼Œç»†ä½“ï¼Œå³å¯¹é½ï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„5%ã€‚åœ¨å›¾ç‰‡å·¦ä¸Šè§’æ·»åŠ è¡¨æƒ…ç¬¦å·ï¼š'ğŸ˜¸ğŸ•'ï¼Œä½¿ç”¨é»‘è‰²å­—ä½“ï¼Œç²—ä½“ï¼Œå·¦å¯¹é½ï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„6%\"}, \"image_2\": {\"description\": \"åˆ›æ„å±…å®¶åŠå…¬è®¾ç½®ï¼ŒåŒ…æ‹¬å¤šå±æ˜¾ç¤ºå™¨ã€äººä½“å·¥å­¦æ¤…ã€ç»¿æ¤è£…é¥°ã€ä»¥åŠå’–å•¡æ¯ç­‰ç”Ÿæ´»åŒ–å…ƒç´ ï¼Œç°ä»£ç®€çº¦é£æ ¼ï¼Œä¸­æ€§è‰²è°ƒï¼Œ45åº¦è§’åº¦æ„å›¾ï¼Œçªå‡ºå·¥ä½œæ•ˆç‡å’Œèˆ’é€‚æ€§\", \"add_on\": \"\"}, \"image_3\": {\"description\": \"è¿œç¨‹å·¥ä½œçš„æ—¶é—´ç®¡ç†å·¥å…·å’ŒæŠ€å·§å±•ç¤ºï¼ŒåŒ…æ‹¬æ—¥ç¨‹è¡¨ã€ç•ªèŒ„é’Ÿã€ä»»åŠ¡æ¸…å•ç­‰ï¼Œå®ç”¨å·¥å…·é£æ ¼ï¼Œè“è‰²ç§‘æŠ€è‰²è°ƒï¼Œä¿¯è§†è§’åº¦æ„å›¾ï¼Œä½“ç°æ—¶é—´ç®¡ç†çš„é‡è¦æ€§\", \"add_on\": \"åœ¨å›¾ç‰‡é¡¶éƒ¨æ·»åŠ æ ‡é¢˜ï¼š'æ—¶é—´ç®¡ç†å¤§å¸ˆ'ï¼Œä½¿ç”¨æ·±è“è‰²å­—ä½“ï¼Œç²—ä½“ï¼Œå±…ä¸­æ˜¾ç¤ºï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„10%ã€‚åœ¨å›¾ç‰‡ä¸­å¤®æ·»åŠ å‰¯æ ‡é¢˜ï¼š'é«˜æ•ˆå·¥ä½œæŠ€å·§'ï¼Œä½¿ç”¨æµ…è“è‰²å­—ä½“ï¼Œä¸­ç­‰ç²—ç»†ï¼Œå±…ä¸­æ˜¾ç¤ºï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„7%ã€‚åœ¨å›¾ç‰‡å³ä¸‹è§’æ·»åŠ æ ‡ç­¾ï¼š'#æ—¶é—´ç®¡ç† #å·¥ä½œæ•ˆç‡'ï¼Œä½¿ç”¨ç™½è‰²å­—ä½“ï¼Œç»†ä½“ï¼Œå³å¯¹é½ï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„5%ã€‚åœ¨å›¾ç‰‡å·¦ä¸‹è§’æ·»åŠ å°æç¤ºï¼š'25åˆ†é’Ÿä¸“æ³¨ï¼Œ5åˆ†é’Ÿä¼‘æ¯'ï¼Œä½¿ç”¨æµ…ç°è‰²å­—ä½“ï¼Œç»†ä½“ï¼Œå·¦å¯¹é½ï¼Œå­—ä½“å¤§å°å å›¾ç‰‡é«˜åº¦çš„4%\"}}",
                        "user_prompt": "ä¸ºå°çº¢ä¹¦åˆ›å»ºä¸€ä¸ªå…³äºè¿œç¨‹å·¥ä½œæŠ€å·§çš„å¹½é»˜å¸–å­ï¼ŒåŒ…å«3å¼ å›¾ç‰‡"
                    }
                }
            ]
        ),
        llm_config=llm_config,
        inputs=[
            {"name": "user_prompt", "type": "string", "description": "ç”¨æˆ·çš„åŸå§‹è¾“å…¥æç¤ºè¯"}
        ],
        outputs=[
            {"name": "research_topic", "type": "string", "description": "æ€»ç»“çš„ç ”ç©¶ä¸»é¢˜"},
            {"name": "image_descriptions_json", "type": "string", "description": "JSONæ ¼å¼çš„å›¾ç‰‡æè¿°å’Œæ–‡å­—å†…å®¹"},
            {"name": "user_prompt", "type": "string", "description": "åŸå§‹ç”¨æˆ·æç¤ºè¯"}
        ]
    )
    
    return prompt_analysis_agent


def create_research_agent():
    """
    Agent 1: Research Agent with Image Analysis
    è´Ÿè´£ä½¿ç”¨ DDGS æœç´¢è·å–çƒ­ç‚¹ä¿¡æ¯å’Œç›¸å…³èµ„æ–™ï¼Œå¹¶åˆ†æå›¾ç‰‡å†…å®¹
    """
    
    # åˆ›å»º DDGS æœç´¢å·¥å…·åŒ…
    all_tools = []
    
    ddgs_toolkit = DDGSSearchToolkit(
        name="DDGSSearchToolkit",
        num_search_pages=5,
        max_content_words=500,
        backend="auto",
        region="cn-zh"
    )
    search_tools = ddgs_toolkit.get_tools()
    all_tools.extend(search_tools)
    
    # æ·»åŠ å›¾ç‰‡åˆ†æå·¥å…·ï¼ˆå¦‚æœæœ‰APIå¯†é’¥ï¼‰
    if OPENROUTER_API_KEY:
        image_analysis_tool = ImageAnalysisTool(
            api_key=OPENROUTER_API_KEY, 
            model="openai/gpt-4o-mini"
        )
        all_tools.append(image_analysis_tool)
    
    research_agent = CustomizeAgent(
        name="SearchResearchAgent",
        description="ä½¿ç”¨DDGSæœç´¢å’Œå›¾ç‰‡åˆ†æåŠŸèƒ½çš„ç½‘ç»œç ”ç©¶ä»£ç†",
        prompt_template=ChatTemplate(
            instruction="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘ç»œç ”ç©¶åŠ©æ‰‹ï¼Œä¸“é—¨ä»äº‹ç¤¾äº¤åª’ä½“å†…å®¹ç ”ç©¶ã€‚ä½ çš„ç›®æ ‡æ˜¯æ”¶é›†å…³äºçƒ­é—¨è¯é¢˜çš„å…¨é¢ä¿¡æ¯ã€‚è¯·ç”¨ä¸­æ–‡å›å¤æ‰€æœ‰å†…å®¹ã€‚",
            context="ä½ å¯ä»¥æœç´¢ç½‘ç»œè·å–å½“å‰ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥åˆ†æå›¾ç‰‡æ¥äº†è§£è§†è§‰å†…å®¹è¶‹åŠ¿ã€‚ä½ å¯ä»¥æ ¹æ®ç ”ç©¶ä¸»é¢˜å’Œå¯ç”¨ä¿¡æ¯æ¥å†³å®šæ˜¯å¦ä½¿ç”¨æœç´¢å·¥å…·ã€‚å¯¹äºéœ€è¦å½“å‰æ•°æ®ã€è¶‹åŠ¿æˆ–æœ€æ–°å‘å±•çš„ä¸»é¢˜ï¼Œä½ åº”è¯¥ä½¿ç”¨ddgs_searchå·¥å…·ã€‚å¯¹äºä½ æœ‰è¶³å¤ŸçŸ¥è¯†æˆ–ä¸éœ€è¦å®æ—¶ä¿¡æ¯çš„ä¸»é¢˜ï¼Œä½ å¯ä»¥ç›´æ¥è¿›è¡Œè€Œä¸æœç´¢ã€‚",
            constraints=[
                "éœ€è¦å½“å‰ä¿¡æ¯æˆ–è¶‹åŠ¿æ—¶ä½¿ç”¨æœç´¢å·¥å…·",
                "æ— è®ºæ˜¯å¦æœç´¢éƒ½è¦æä¾›å‡†ç¡®ä¿¡æ¯",
                "æ‰€æœ‰è¾“å‡ºå†…å®¹å¿…é¡»ä½¿ç”¨ä¸­æ–‡",
                "ä¸è¦è¿”å›research topicçš„å†…å®¹"
            ],
            demonstrations=[]
        ),
        llm_config=llm_config,
        inputs=[
            {"name": "research_topic", "type": "string", "description": "The topic to research"},
            {"name": "platform", "type": "string", "description": "Target social media platform"}
        ],
        outputs=[
            {"name": "research_info", "type": "string", "description": "Comprehensive research report with text and visual insights"}
        ],
        tools=all_tools
    )
    
    return research_agent

def create_content_generation_agent():
    """
    Agent 2: Content Generation Agent
    åŸºäºç ”ç©¶ä¿¡æ¯ç”Ÿæˆç¤¾äº¤åª’ä½“æ¨æ–‡å†…å®¹
    """
    
    content_agent = CustomizeAgent(
        name="ContentGenerationAgent", 
        description="ç¤¾äº¤åª’ä½“å†…å®¹åˆ›ä½œä¸“å®¶",
        prompt_template=ChatTemplate(
            instruction="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¤¾äº¤åª’ä½“å†…å®¹åˆ›ä½œè€…ï¼Œè´Ÿè´£å°†ç ”ç©¶æ´å¯Ÿè½¬åŒ–ä¸ºå¼•äººå…¥èƒœã€å…·æœ‰ä¼ æ’­ä»·å€¼çš„å†…å®¹ã€‚è¯·ç”¨ä¸­æ–‡åˆ›ä½œæ‰€æœ‰å†…å®¹ã€‚",
            context="ä½ ä¸“é—¨åˆ›ä½œé’ˆå¯¹ç‰¹å®šå¹³å°çš„å†…å®¹ï¼Œä»¥æå‡ç”¨æˆ·å‚ä¸åº¦ã€‚ä½ äº†è§£å½“å‰ç¤¾äº¤åª’ä½“è¶‹åŠ¿ã€ç®—æ³•åå¥½å’Œå—ä¼—å¿ƒç†ã€‚",
            constraints=[
                "å†…å®¹å¿…é¡»åŸåˆ›ä¸”çœŸå®",
                "ç¬¦åˆæŒ‡å®šçš„é£æ ¼å’Œå¹³å°è¦æ±‚",
                "åŒ…å«å¼•äººå…¥èƒœçš„å¼€å¤´å’Œæ¸…æ™°çš„è¡ŒåŠ¨å·å¬",
                "æ‰€æœ‰è¾“å‡ºå†…å®¹å¿…é¡»ä½¿ç”¨ä¸­æ–‡"
            ],
            demonstrations=[]
        ),
        llm_config=llm_config,
        inputs=[
            {"name": "research_info", "type": "string", "description": "Research information from previous step"},
        ],
        outputs=[
            {"name": "post_content", "type": "string", "description": "Generated social media post content"}
        ]
    )
    
    return content_agent

def create_image_generation_agent(save_path: str = "./social_media_output"):
    """
    Agent 3: Image Generation Agent (ä½¿ç”¨ActionAgentå®ç°FluxOpenAIEditingActionAgent)
    åŸºäºå†…å®¹ç”Ÿæˆé…å¥—çš„ç¤¾äº¤åª’ä½“å›¾ç‰‡ï¼Œæ”¯æŒFluxç”Ÿæˆå’ŒOpenAIæ–‡å­—ç¼–è¾‘
    
    Args:
        save_path: ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸º"./social_media_output"
    """
    
    def execute_flux_openai_editing(image_descriptions_json: str) -> dict:
        """
        æ‰§è¡ŒFlux to OpenAI Editingæµç¨‹
        
        Args:
            image_descriptions_json: JSONæ ¼å¼çš„å›¾ç‰‡æè¿°ï¼Œæ¥è‡ªxhs workflowçš„analyze agent
            
        Returns:
            dict: åŒ…å«å›¾ç‰‡è·¯å¾„ä¿¡æ¯çš„å­—å…¸
        """
        import json
        import base64
        
        # éªŒè¯ç¯å¢ƒå˜é‡
        if not all([OPENAI_API_KEY, OPENAI_ORGANIZATION_ID, BFL_API_KEY]):
            raise ValueError("è¯·è®¾ç½®OPENAI_API_KEYã€OPENAI_ORGANIZATION_IDå’ŒBFL_API_KEYç¯å¢ƒå˜é‡")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_path, exist_ok=True)
        
        try:
            # è§£æJSONè¾“å…¥ - ä½¿ç”¨æ›´è¯¦ç»†çš„è§£ææ–¹å¼
            if isinstance(image_descriptions_json, str):
                # å¤„ç†å¯èƒ½åŒ…å«markdownä»£ç å—çš„JSONå­—ç¬¦ä¸²
                json_string = image_descriptions_json.strip()
                
                # å¦‚æœåŒ…å«markdownä»£ç å—ï¼Œæå–å…¶ä¸­çš„JSONå†…å®¹
                if json_string.startswith('```'):
                    # ç§»é™¤markdownä»£ç å—æ ‡è®°
                    lines = json_string.split('\n')
                    json_lines = []
                    in_json = False
                    for line in lines:
                        if line.strip().startswith('```'):
                            if not in_json:
                                in_json = True
                            else:
                                break
                        elif in_json:
                            json_lines.append(line)
                    json_string = '\n'.join(json_lines)
                
                # è§£æJSON
                try:
                    image_descriptions = json.loads(json_string)
                except json.JSONDecodeError as e:
                    raise ValueError(f"JSONè§£æå¤±è´¥: {str(e)}ï¼ŒåŸå§‹å†…å®¹: {image_descriptions_json[:200]}...")
            else:
                image_descriptions = image_descriptions_json
            
            # éªŒè¯JSONç»“æ„
            if not isinstance(image_descriptions, dict):
                raise ValueError("image_descriptions_jsonå¿…é¡»æ˜¯åŒ…å«å›¾ç‰‡æè¿°çš„å­—å…¸")
            
            result = {}
            
            # å¤„ç†æ¯ä¸ªå›¾ç‰‡æè¿°
            for image_key, image_info in image_descriptions.items():
                if not isinstance(image_info, dict):
                    print(f"è­¦å‘Š: {image_key} ä¸æ˜¯æœ‰æ•ˆçš„å­—å…¸æ ¼å¼ï¼Œè·³è¿‡")
                    continue
                
                description = image_info.get("description", "")
                add_on = image_info.get("add_on", "")
                
                if not description:
                    print(f"è­¦å‘Š: {image_key} ç¼ºå°‘descriptionå­—æ®µ")
                    continue
                
                print(f"å¼€å§‹å¤„ç† {image_key}: {description}")
                
                # ä½¿ç”¨Fluxç”Ÿæˆå›¾ç‰‡
                flux_tool = FluxImageGenerationTool(api_key=BFL_API_KEY, save_path=save_path)
                flux_result = flux_tool(prompt=description)
                generated_image_path = flux_result.get("file_path")
                
                if not generated_image_path or not os.path.exists(generated_image_path):
                    raise Exception(f"Fluxå›¾ç‰‡ç”Ÿæˆå¤±è´¥: {image_key}")
                
                print(f"Fluxç”Ÿæˆçš„å›¾ç‰‡å·²ä¿å­˜åˆ°: {generated_image_path}")
                
                # å¦‚æœæœ‰add_onä¿¡æ¯ï¼Œä½¿ç”¨OpenAIæ·»åŠ æ–‡å­—
                if add_on:
                    from openai import OpenAI
                    
                    # åˆ›å»º OpenAI å®¢æˆ·ç«¯
                    client = OpenAI(api_key=OPENAI_API_KEY, organization=OPENAI_ORGANIZATION_ID)
                    
                    # ç¼–è¾‘å›¾ç‰‡
                    response = client.images.edit(
                        model="gpt-image-1",
                        image=open(generated_image_path, "rb"),
                        prompt=add_on
                    )
                    
                    # è®¾ç½®è¾“å‡ºæ–‡ä»¶å
                    output_name = f"{image_key}_edited.jpeg"
                    edited_image_path = os.path.join(save_path, output_name)
                    
                    # ä¿å­˜ç¼–è¾‘åçš„å›¾ç‰‡
                    image_base64 = response.data[0].b64_json
                    image_bytes = base64.b64decode(image_base64)
                    
                    with open(edited_image_path, "wb") as f:
                        f.write(image_bytes)
                    
                    print(f"âœ… OpenAIæ·»åŠ æ–‡å­—å®Œæˆï¼ä¿å­˜åœ¨: {edited_image_path}")
                    
                    result[image_key] = {
                        "generated_image_path": generated_image_path,
                        "edited_image_path": edited_image_path,
                        "description": description,
                        "add_on": add_on
                    }
                else:
                    result[image_key] = {
                        "generated_image_path": generated_image_path,
                        "edited_image_path": generated_image_path,  # æ²¡æœ‰ç¼–è¾‘æ—¶ï¼Œä½¿ç”¨åŸå›¾è·¯å¾„
                        "description": description,
                        "add_on": ""
                    }
            
            return {"image_paths_json": json.dumps(result, ensure_ascii=False, indent=2)}
            
        except Exception as e:
            print(f"æ‰§è¡ŒFlux to OpenAI Editingæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            error_result = {
                "error": f"æ‰§è¡ŒFlux to OpenAI Editingæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "status": "failed"
            }
            return {"image_paths_json": json.dumps(error_result, ensure_ascii=False, indent=2)}
    
    return ActionAgent(
        name="FluxOpenAIEditingAgent",
        description="åŸºäºFluxç”Ÿæˆå›¾ç‰‡å¹¶ä½¿ç”¨OpenAIæ·»åŠ æ–‡å­—çš„Action Agentï¼Œä¸“é—¨å¤„ç†xhs workflowä¸­çš„å›¾ç‰‡ç”Ÿæˆéœ€æ±‚",
        inputs=[
            {
                "name": "image_descriptions_json",
                "type": "string",
                "description": "JSONæ ¼å¼çš„å›¾ç‰‡æè¿°ï¼Œæ¥è‡ªxhs workflowçš„analyze agentï¼ŒåŒ…å«å›¾ç‰‡æè¿°å’Œéœ€è¦æ·»åŠ çš„æ–‡å­—å†…å®¹",
                "required": True
            }
        ],
        outputs=[
            {
                "name": "image_paths_json",
                "type": "string",
                "description": "JSONæ ¼å¼çš„å›¾ç‰‡è·¯å¾„ä¿¡æ¯ï¼ŒåŒ…å«æ¯ä¸ªå›¾ç‰‡çš„ç”Ÿæˆè·¯å¾„å’Œç¼–è¾‘åçš„è·¯å¾„",
                "required": True
            }
        ],
        execute_func=execute_flux_openai_editing
    )

def create_post_content_writer_agent(save_path: str = "./social_media_output"):
    """
    Agent 4: Post Content Writer Agent (ä½¿ç”¨ActionAgent)
    è´Ÿè´£å°†ç”Ÿæˆçš„ç¤¾äº¤åª’ä½“å†…å®¹å†™å…¥æ–‡ä»¶
    
    Args:
        save_path: ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸º"./social_media_output"
    """
    
    def execute_post_content_writer(post_content: str) -> dict:
        """
        æ‰§è¡Œpost contentå†™å…¥åŠŸèƒ½
        
        Args:
            post_content: è¦å†™å…¥çš„post content
            
        Returns:
            dict: åŒ…å«å†™å…¥ç»“æœçš„å­—å…¸
        """
        import json
        
        try:
            # åˆ›å»ºä¿å­˜ç›®å½•
            os.makedirs(save_path, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"post_content_{timestamp}.txt"
            filepath = os.path.join(save_path, filename)
            
            # å†™å…¥æ–‡ä»¶
            with open(filepath, "w", encoding="utf-8") as f:
                f.write("=== ç¤¾äº¤åª’ä½“å†…å®¹ ===\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("\n" + "="*50 + "\n")
                f.write(post_content)
                f.write("\n\n" + "="*50 + "\n")
                f.write("å†…å®¹ç”Ÿæˆå®Œæˆ")
            
            result = {
                "status": "success",
                "file_path": filepath,
                "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "content_length": len(post_content)
            }
            
            print(f"âœ… Post contentå·²ä¿å­˜åˆ°: {filepath}")
            return {"post_content_result": json.dumps(result, ensure_ascii=False, indent=2)}
            
        except Exception as e:
            print(f"âŒ å†™å…¥post contentæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            error_result = {
                "status": "failed",
                "error": f"å†™å…¥post contentæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            return {"post_content_result": json.dumps(error_result, ensure_ascii=False, indent=2)}
    
    return ActionAgent(
        name="PostContentWriterAgent",
        description="ä¸“é—¨è´Ÿè´£æ¥æ”¶post contentå¹¶å†™å…¥æ–‡ä»¶çš„Action Agent",
        inputs=[
            {
                "name": "post_content",
                "type": "string",
                "description": "è¦å†™å…¥çš„post contentå†…å®¹",
                "required": True
            }
        ],
        outputs=[
            {
                "name": "post_content_result",
                "type": "string",
                "description": "JSONæ ¼å¼çš„å†™å…¥ç»“æœä¿¡æ¯ï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„å’ŒçŠ¶æ€",
                "required": True
            }
        ],
        execute_func=execute_post_content_writer
    )

def create_social_media_workflow(save_path: str = "./social_media_output", load_from_file: str = None):
    """
    åˆ›å»ºå®Œæ•´çš„ç¤¾äº¤åª’ä½“å†…å®¹ç”Ÿæˆå·¥ä½œæµ
    åŒ…å«å››ä¸ªAgentï¼šPrompt Analysis -> Research -> Content Generation -> Image Generation
    
    Args:
        save_path: ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸º"./social_media_output"
    """
    
    # 1. åˆ›å»ºå››ä¸ªAgent
    prompt_analysis_agent = create_prompt_analysis_agent()
    research_agent = create_research_agent()
    content_agent = create_content_generation_agent()
    image_agent = create_image_generation_agent(save_path)
    post_writer_agent = create_post_content_writer_agent(save_path)
    
    # 2. åˆ›å»ºå·¥ä½œæµèŠ‚ç‚¹
    nodes = [
        WorkFlowNode(
            name="prompt_analysis",
            description="Analyze user prompt to determine research topic and image requirements",
            agents=[prompt_analysis_agent],
            inputs=[
                {"name": "user_prompt", "type": "string", "description": "User's original input", "required": True}
            ],
            outputs=[
                {"name": "research_topic", "type": "string", "description": "Research topic", "required": True},
                {"name": "image_descriptions_json", "type": "string", "description": "JSONæ ¼å¼çš„å›¾ç‰‡æè¿°å’Œé™„åŠ ä¿¡æ¯", "required": True},
                {"name": "user_prompt", "type": "string", "description": "Original user prompt", "required": True}
            ]
        ),
        WorkFlowNode(
            name="research",
            description="Research trending topics using DDGS search",
            agents=[research_agent],
            inputs=[
                {"name": "research_topic", "type": "string", "description": "Topic to research", "required": True},
                {"name": "platform", "type": "string", "description": "Target platform", "required": True}
            ],
            outputs=[
                {"name": "research_info", "type": "string", "description": "Research findings", "required": True}
            ]
        ),
        WorkFlowNode(
            name="content_generation",
            description="Generate social media content based on research",
            agents=[content_agent],
            inputs=[
                {"name": "research_info", "type": "string", "description": "Research information", "required": True},
                {"name": "user_prompt", "type": "string", "description": "User input prompt", "required": True}
            ],
            outputs=[
                {"name": "post_content", "type": "string", "description": "Generated content", "required": True}
            ]
        ),
        WorkFlowNode(
            name="image_generation",
            description="Generate images for social media content using Flux and OpenAI editing",
            agents=[image_agent],
            inputs=[
                {"name": "image_descriptions_json", "type": "string", "description": "JSONæ ¼å¼çš„å›¾ç‰‡æè¿°ï¼Œæ¥è‡ªprompt_analysisèŠ‚ç‚¹", "required": True}
            ],
            outputs=[
                {"name": "image_paths_json", "type": "string", "description": "JSONæ ¼å¼çš„å›¾ç‰‡è·¯å¾„ä¿¡æ¯ï¼ŒåŒ…å«æ¯ä¸ªå›¾ç‰‡çš„ç”Ÿæˆè·¯å¾„å’Œç¼–è¾‘åçš„è·¯å¾„", "required": True}
            ]
        ),
        WorkFlowNode(
            name="post_content_writer",
            description="Write post content to file",
            agents=[post_writer_agent],
            inputs=[
                {"name": "post_content", "type": "string", "description": "Generated post content from content_generation node", "required": True}
            ],
            outputs=[
                {"name": "post_content_result", "type": "string", "description": "JSONæ ¼å¼çš„å†™å…¥ç»“æœä¿¡æ¯", "required": True}
            ]
        )
    ]
    
    # 3. å®šä¹‰å·¥ä½œæµè¾¹ï¼ˆæ•°æ®æµå‘ï¼‰
    edges = [
        # Prompt Analysis -> Research
        WorkFlowEdge(source="prompt_analysis", target="research"),
        # Prompt Analysis -> Content Generation (ä¼ é€’user_prompt)
        WorkFlowEdge(source="prompt_analysis", target="content_generation"),
        # Prompt Analysis -> Image Generation (ä¼ é€’image_descriptions_json)
        WorkFlowEdge(source="prompt_analysis", target="image_generation"),
        # Research -> Content Generation
        WorkFlowEdge(source="research", target="content_generation"),
        # Content Generation -> Post Content Writer
        WorkFlowEdge(source="content_generation", target="post_content_writer")
    ]
    
    # 4. åˆ›å»ºå·¥ä½œæµå›¾
    if load_from_file and os.path.exists(load_from_file):
        print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½workflow: {load_from_file}")
        graph = WorkFlowGraph.from_file(load_from_file)
    else:
        graph = WorkFlowGraph(
            goal="Create social media content with prompt analysis, research, content generation, and image creation",
            nodes=nodes,
            edges=edges
        )
    
    # 5. åˆ›å»ºAgent Manager
    agents = [prompt_analysis_agent, research_agent, content_agent, image_agent, post_writer_agent]
    agent_manager = AgentManager(agents=agents)
    
    # 6. åˆ›å»ºå®Œæ•´å·¥ä½œæµ
    workflow = WorkFlow(
        graph=graph,
        llm= OpenAILLM(llm_config),
        agent_manager=agent_manager
    )
    
    # ä¿å­˜workflowé…ç½®åˆ°JSONæ–‡ä»¶
    os.makedirs("examples/output", exist_ok=True)
    save_path = "examples/output/social_media_workflow.json"
    graph.save_module(save_path)
    print(f"âœ… Workflowå·²ä¿å­˜åˆ°: {save_path}")
    
    return workflow
